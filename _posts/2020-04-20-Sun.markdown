---
title: "SPADnet: Deep RGB-SPAD sensor fusion assisted by monocular depth estimation"
subtitle: Optics Express (2020)
brief: A neural sensor fusion framework for robust 3D imaging with single-photon detectors.
authors: Zhanghao Sun, <strong>David B. Lindell</strong>, Olav Solgaard, Gordon Wetzstein
layout: pub 
date: 2020-04-20
img: oe2020sun.png  
thumbnail: oe2020sun.png  
permalink: publications/spadnet
alt: image-alt
pdf-link: https://www.osapublishing.org/oe/abstract.cfm?uri=oe-28-10-14948
code-link: https://github.com/zhsun0357/SPADNet

---

### Abstract
- - -
Single-photon light detection and ranging (LiDAR) techniques use emerging single-photon detectors (SPADs) to push 3D imaging capabilities to unprecedented ranges. However, it remains challenging to robustly estimate scene depth from the noisy and otherwise corrupted measurements recorded by a SPAD. Here, we propose a deep sensor fusion strategy that combines corrupted SPAD data and a conventional 2D image to estimate the depth of a scene. Our primary contribution is a neural network architecture—SPADnet—that uses a monocular depth estimation algorithm together with a SPAD denoising and sensor fusion strategy. This architecture, together with several techniques in network training, achieves state-of-the-art results for RGB-SPAD fusion with simulated and captured data. Moreover, SPADnet is more computationally efficient than previous RGB-SPAD fusion networks.
{: style="text-align: left;" }

### Citation
- - -
```
{% raw %}
@article{sun2020spadnet,
  title={{SPADnet}: deep {RGB-SPAD} sensor fusion assisted by monocular depth estimation},
  author={Sun, Zhanghao and Lindell, David B and Solgaard, Olav and Wetzstein, Gordon},
  journal={Optics Express},
  volume={28},
  number={10},
  pages={14948--14962},
  year={2020},
  publisher={Optical Society of America}
}
{% endraw %}
```
{: style="text-align: left;" }
